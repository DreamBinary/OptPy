{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-06T05:48:22.544335100Z",
     "start_time": "2024-02-06T05:48:22.490222200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 device(s) found.\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "print(\"%d device(s) found.\" % cuda.Device.count())"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA GeForce RTX 2060 with Max-Q Design\n",
      "  Compute Capability: 7.5\n",
      "  Total Memory: 6291136 KB\n",
      "  ASYNC_ENGINE_COUNT: 2\n",
      "  CAN_MAP_HOST_MEMORY: 1\n",
      "  CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM: 0\n",
      "  CLOCK_RATE: 1185000\n",
      "  COMPUTE_CAPABILITY_MAJOR: 7\n",
      "  COMPUTE_CAPABILITY_MINOR: 5\n",
      "  COMPUTE_MODE: DEFAULT\n",
      "  COMPUTE_PREEMPTION_SUPPORTED: 1\n",
      "  CONCURRENT_KERNELS: 1\n",
      "  CONCURRENT_MANAGED_ACCESS: 0\n",
      "  DIRECT_MANAGED_MEM_ACCESS_FROM_HOST: 0\n",
      "  ECC_ENABLED: 0\n",
      "  GENERIC_COMPRESSION_SUPPORTED: 0\n",
      "  GLOBAL_L1_CACHE_SUPPORTED: 1\n",
      "  GLOBAL_MEMORY_BUS_WIDTH: 192\n",
      "  GPU_OVERLAP: 1\n",
      "  HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED: 1\n",
      "  HANDLE_TYPE_WIN32_HANDLE_SUPPORTED: 0\n",
      "  HANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED: 0\n",
      "  HOST_NATIVE_ATOMIC_SUPPORTED: 0\n",
      "  INTEGRATED: 0\n",
      "  KERNEL_EXEC_TIMEOUT: 1\n",
      "  L2_CACHE_SIZE: 3145728\n",
      "  LOCAL_L1_CACHE_SUPPORTED: 1\n",
      "  MANAGED_MEMORY: 1\n",
      "  MAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048\n",
      "  MAXIMUM_SURFACE1D_LAYERED_WIDTH: 32768\n",
      "  MAXIMUM_SURFACE1D_WIDTH: 32768\n",
      "  MAXIMUM_SURFACE2D_HEIGHT: 65536\n",
      "  MAXIMUM_SURFACE2D_LAYERED_HEIGHT: 32768\n",
      "  MAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048\n",
      "  MAXIMUM_SURFACE2D_LAYERED_WIDTH: 32768\n",
      "  MAXIMUM_SURFACE2D_WIDTH: 131072\n",
      "  MAXIMUM_SURFACE3D_DEPTH: 16384\n",
      "  MAXIMUM_SURFACE3D_HEIGHT: 16384\n",
      "  MAXIMUM_SURFACE3D_WIDTH: 16384\n",
      "  MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046\n",
      "  MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 32768\n",
      "  MAXIMUM_SURFACECUBEMAP_WIDTH: 32768\n",
      "  MAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048\n",
      "  MAXIMUM_TEXTURE1D_LAYERED_WIDTH: 32768\n",
      "  MAXIMUM_TEXTURE1D_LINEAR_WIDTH: 268435456\n",
      "  MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 32768\n",
      "  MAXIMUM_TEXTURE1D_WIDTH: 131072\n",
      "  MAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 32768\n",
      "  MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048\n",
      "  MAXIMUM_TEXTURE2D_ARRAY_WIDTH: 32768\n",
      "  MAXIMUM_TEXTURE2D_GATHER_HEIGHT: 32768\n",
      "  MAXIMUM_TEXTURE2D_GATHER_WIDTH: 32768\n",
      "  MAXIMUM_TEXTURE2D_HEIGHT: 65536\n",
      "  MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65000\n",
      "  MAXIMUM_TEXTURE2D_LINEAR_PITCH: 2097120\n",
      "  MAXIMUM_TEXTURE2D_LINEAR_WIDTH: 131072\n",
      "  MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 32768\n",
      "  MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 32768\n",
      "  MAXIMUM_TEXTURE2D_WIDTH: 131072\n",
      "  MAXIMUM_TEXTURE3D_DEPTH: 16384\n",
      "  MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 32768\n",
      "  MAXIMUM_TEXTURE3D_HEIGHT: 16384\n",
      "  MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 8192\n",
      "  MAXIMUM_TEXTURE3D_WIDTH: 16384\n",
      "  MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 8192\n",
      "  MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046\n",
      "  MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 32768\n",
      "  MAXIMUM_TEXTURECUBEMAP_WIDTH: 32768\n",
      "  MAX_BLOCKS_PER_MULTIPROCESSOR: 16\n",
      "  MAX_BLOCK_DIM_X: 1024\n",
      "  MAX_BLOCK_DIM_Y: 1024\n",
      "  MAX_BLOCK_DIM_Z: 64\n",
      "  MAX_GRID_DIM_X: 2147483647\n",
      "  MAX_GRID_DIM_Y: 65535\n",
      "  MAX_GRID_DIM_Z: 65535\n",
      "  MAX_PERSISTING_L2_CACHE_SIZE: 0\n",
      "  MAX_PITCH: 2147483647\n",
      "  MAX_REGISTERS_PER_BLOCK: 65536\n",
      "  MAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n",
      "  MAX_SHARED_MEMORY_PER_BLOCK: 49152\n",
      "  MAX_SHARED_MEMORY_PER_BLOCK_OPTIN: 65536\n",
      "  MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536\n",
      "  MAX_THREADS_PER_BLOCK: 1024\n",
      "  MAX_THREADS_PER_MULTIPROCESSOR: 1024\n",
      "  MEMORY_CLOCK_RATE: 5501000\n",
      "  MEMORY_POOLS_SUPPORTED: 1\n",
      "  MULTIPROCESSOR_COUNT: 30\n",
      "  MULTI_GPU_BOARD: 0\n",
      "  MULTI_GPU_BOARD_GROUP_ID: 0\n",
      "  PAGEABLE_MEMORY_ACCESS: 0\n",
      "  PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES: 0\n",
      "  PCI_BUS_ID: 1\n",
      "  PCI_DEVICE_ID: 0\n",
      "  PCI_DOMAIN_ID: 0\n",
      "  READ_ONLY_HOST_REGISTER_SUPPORTED: 1\n",
      "  RESERVED_SHARED_MEMORY_PER_BLOCK: 0\n",
      "  SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO: 32\n",
      "  STREAM_PRIORITIES_SUPPORTED: 1\n",
      "  SURFACE_ALIGNMENT: 512\n",
      "  TCC_DRIVER: 0\n",
      "  TEXTURE_ALIGNMENT: 512\n",
      "  TEXTURE_PITCH_ALIGNMENT: 32\n",
      "  TOTAL_CONSTANT_MEMORY: 65536\n",
      "  UNIFIED_ADDRESSING: 1\n",
      "  WARP_SIZE: 32\n"
     ]
    }
   ],
   "source": [
    "for i in range(cuda.Device.count()):\n",
    "    dev = cuda.Device(i)\n",
    "    print(\"Device %d: %s\" % (i, dev.name()))\n",
    "    print(\"  Compute Capability: %d.%d\" % dev.compute_capability())\n",
    "    print(\"  Total Memory: %s KB\" % (dev.total_memory() // (1024)))\n",
    "    atts = [(str(att), value) for att, value in dev.get_attributes().items()]\n",
    "    atts.sort()\n",
    "    for att, value in atts:\n",
    "        print(\"  %s: %s\" % (att, value))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T05:48:33.829492900Z",
     "start_time": "2024-02-06T05:48:33.742774900Z"
    }
   },
   "id": "14d59b2ed3ce137",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "thread -->> block -->> grid\n",
    "sp     -->> sm    -->> device\n",
    "wrap is a group of 32 threads in a block"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f49815e0c606e0cc"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5788051  -1.2548146  -1.2591091   0.6281901 ]\n",
      " [ 1.2621292  -0.4714631   0.05602671 -1.0856785 ]\n",
      " [ 1.060138   -0.4933093  -0.3339168   0.33785075]\n",
      " [ 1.07119     0.6540993   1.1726604  -0.5625655 ]]\n",
      "[[-1.1576102  -2.5096292  -2.5182183   1.2563802 ]\n",
      " [ 2.5242584  -0.9429262   0.11205342 -2.171357  ]\n",
      " [ 2.120276   -0.9866186  -0.6678336   0.6757015 ]\n",
      " [ 2.14238     1.3081986   2.3453207  -1.125131  ]]\n",
      "CPU times: user 2.93 ms, sys: 0 ns, total: 2.93 ms\n",
      "Wall time: 2.27 ms\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_gpu():\n",
    "    arr = np.random.randn(4, 4)\n",
    "    arr = arr.astype(np.float32)\n",
    "\n",
    "    arr_gpu = cuda.mem_alloc(arr.nbytes)\n",
    "    cuda.memcpy_htod(arr_gpu, arr)  # host to device\n",
    "    mod = SourceModule(\"\"\"\n",
    "        __global__ void doublify(float *a)\n",
    "        {\n",
    "            int idx = threadIdx.x + threadIdx.y*4;\n",
    "            a[idx] *= 2;\n",
    "        }\n",
    "        \"\"\")\n",
    "    # __global__ means that the function is called from the host and runs on the device\n",
    "    # threadIdx is a built-in CUDA variable that holds the thread index within the block\n",
    "    # blockIdx is a built-in CUDA variable that holds the block index within the grid\n",
    "    # why 4? because we have 4x4 array\n",
    "    func = mod.get_function(\"doublify\")\n",
    "    func(arr_gpu, block=(4, 4, 1))\n",
    "    # block is a tuple of 3 integers that specifies the dimensions of the block\n",
    "    # grid is a tuple of 3 integers that specifies the dimensions of the grid\n",
    "    # dim x has 4 threads, dim y has 4 threads, dim z has 1 thread\n",
    "    arr_doubled = np.empty_like(arr)\n",
    "    cuda.memcpy_dtoh(arr_doubled, arr_gpu)\n",
    "    print(arr)\n",
    "    print(arr_doubled)\n",
    "\n",
    "%time test_gpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T05:58:02.092588900Z",
     "start_time": "2024-02-06T05:58:02.035802Z"
    }
   },
   "id": "3b23f4685ef6a040",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19378187  1.0278679  -1.1469375   0.1396176 ]\n",
      " [ 1.5364314   0.13483281 -0.7757695   0.6898246 ]\n",
      " [-0.4454204  -0.6694119  -0.49060008 -1.7487348 ]\n",
      " [ 0.28382403 -0.46344396 -0.9994953  -0.17561226]]\n",
      "[[-0.38756374  2.0557358  -2.293875    0.2792352 ]\n",
      " [ 3.0728629   0.26966563 -1.551539    1.3796492 ]\n",
      " [-0.8908408  -1.3388238  -0.98120016 -3.4974697 ]\n",
      " [ 0.56764805 -0.9268879  -1.9989907  -0.3512245 ]]\n",
      "CPU times: user 1.15 ms, sys: 255 Âµs, total: 1.4 ms\n",
      "Wall time: 1.35 ms\n"
     ]
    }
   ],
   "source": [
    "def test_cpu():\n",
    "    arr = np.random.randn(4, 4)\n",
    "    arr = arr.astype(np.float32)\n",
    "    arr_doubled = arr * 2\n",
    "    print(arr)\n",
    "    print(arr_doubled)\n",
    "\n",
    "%time test_cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T05:58:02.702600600Z",
     "start_time": "2024-02-06T05:58:02.696068700Z"
    }
   },
   "id": "e6938a939895b652",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "import numpy\n",
    "\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "__global__ void multiply_them(float *dest, float *a, float *b)\n",
    "{\n",
    "  const int i = threadIdx.x;\n",
    "  dest[i] = a[i] * b[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "multiply_them = mod.get_function(\"multiply_them\")\n",
    "\n",
    "a = numpy.random.randn(400).astype(numpy.float32)\n",
    "b = numpy.random.randn(400).astype(numpy.float32)\n",
    "\n",
    "dest = numpy.zeros_like(a)\n",
    "multiply_them(\n",
    "    drv.Out(dest), drv.In(a), drv.In(b),\n",
    "    block=(400, 1, 1), grid=(1, 1))\n",
    "\n",
    "print(dest - a * b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T06:09:43.650876200Z",
     "start_time": "2024-02-06T06:09:43.038712Z"
    }
   },
   "id": "22559353eee26e9f",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.4968941   0.02882253  2.7008696   0.9127767 ]\n",
      " [-0.4148375   3.3071177   1.6443638  -0.8705558 ]\n",
      " [-1.5854768  -2.152403    2.9916284  -2.2097363 ]\n",
      " [ 1.045465    2.6737626   0.12718704 -1.452502  ]]\n",
      "[[ 1.2484471   0.01441126  1.3504348   0.45638835]\n",
      " [-0.20741875  1.6535589   0.8221819  -0.4352779 ]\n",
      " [-0.7927384  -1.0762016   1.4958142  -1.1048682 ]\n",
      " [ 0.5227325   1.3368813   0.06359352 -0.726251  ]]\n",
      "CPU times: user 14.9 ms, sys: 8.86 ms, total: 23.8 ms\n",
      "Wall time: 453 ms\n"
     ]
    }
   ],
   "source": [
    "import pycuda.gpuarray as gpuarray\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy\n",
    "\n",
    "\n",
    "def test_gpu():\n",
    "    a_gpu = gpuarray.to_gpu(numpy.random.randn(4, 4).astype(numpy.float32))\n",
    "    a_doubled = (2 * a_gpu).get()\n",
    "    print(a_doubled)\n",
    "    print(a_gpu)\n",
    "\n",
    "%time test_gpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T06:16:58.800409500Z",
     "start_time": "2024-02-06T06:16:58.300898400Z"
    }
   },
   "id": "f626268fafbf1f0b",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__global__ void matrix_mul(float *a, float *b, float *c)\n",
      "{\n",
      "    int row = threadIdx.y;\n",
      "    int col = threadIdx.x;\n",
      "    float sum = 0;\n",
      "    for (int i = 0; i < 4; i++)\n",
      "    {\n",
      "        sum += a[row*4+i] * b[i*4+col];\n",
      "    }\n",
      "    c[row*4+col] = sum;\n",
      "}\n",
      "\n",
      "[[-2.3841858e-07  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  1.1920929e-07  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 -2.9802322e-08]\n",
      " [-2.9802322e-08  0.0000000e+00  1.4901161e-08  0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "void maxtrix_mul(float *a, float *b, float *c, int n)\n",
    "{\n",
    "    for (int i = 0; i < n; i++)\n",
    "    {\n",
    "        for (int j = 0; j < n; j++)\n",
    "        {\n",
    "            float sum = 0;\n",
    "            for (int k = 0; k < n; k++)\n",
    "            {\n",
    "                sum += a[i*n+k] * b[k*n+j];\n",
    "            }\n",
    "            c[i*n+j] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# __global__ void matrix_mul(float *a, float *b, float *c)\n",
    "# {\n",
    "#     int row = threadIdx.y;\n",
    "#     int col = threadIdx.x;\n",
    "#     float sum = 0;\n",
    "#     for (int i = 0; i < 4; i++)\n",
    "#     {\n",
    "#         sum += a[row*4+i] * b[i*4+col];\n",
    "#     }\n",
    "#     c[row*4+col] = sum;\n",
    "# }\n",
    "\n",
    "\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "import numpy as np\n",
    "\n",
    "MATRIX_SIZE = 4\n",
    "\n",
    "kernel_code = f\"\"\"\n",
    "__global__ void matrix_mul(float *a, float *b, float *c)\n",
    "{{\n",
    "    int row = threadIdx.y;\n",
    "    int col = threadIdx.x;\n",
    "    float sum = 0;\n",
    "    for (int i = 0; i < {MATRIX_SIZE}; i++)\n",
    "    {{\n",
    "        sum += a[row*{MATRIX_SIZE}+i] * b[i*{MATRIX_SIZE}+col];\n",
    "    }}\n",
    "    c[row*{MATRIX_SIZE}+col] = sum;\n",
    "}}\n",
    "\"\"\"\n",
    "print(kernel_code)\n",
    "# why {{ and }}? because we want to use { and } as a part of the string\n",
    "# %(MATRIX_SIZE)s is a placeholder for the value of MATRIX_SIZE\n",
    "# why s? because it is a string\n",
    "\n",
    "mod = compiler.SourceModule(kernel_code)\n",
    "\n",
    "a_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32)\n",
    "b_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32)\n",
    "c_cpu = np.dot(a_cpu, b_cpu)\n",
    "a_gpu = gpuarray.to_gpu(a_cpu)\n",
    "b_gpu = gpuarray.to_gpu(b_cpu)\n",
    "c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32)\n",
    "matrix_mul = mod.get_function(\"matrix_mul\")\n",
    "\n",
    "matrix_mul(a_gpu, b_gpu, c_gpu, block=(MATRIX_SIZE, MATRIX_SIZE, 1))\n",
    "print(c_gpu.get() - c_cpu)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T06:42:46.882128300Z",
     "start_time": "2024-02-06T06:42:46.828496800Z"
    }
   },
   "id": "26bc0ece31848def",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d050939d9fac372d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
